{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JGA--2lOhgW",
        "outputId": "6481cb31-b034-4f85-905b-6fd84a012c04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Reading Train 60000.cdb ...\n",
            "Train Dataset Loaded\n",
            "Reading Test 20000.cdb ...\n",
            "Test Dataset Loaded\n",
            "[11199, 18848, 5445, 17962, 2467, 11944, 9103, 18237, 16335, 18323, 9560, 1139, 2457, 11889, 15706, 1165, 6687, 16726, 3722, 6019, 10281, 2490, 5785, 10142, 14460, 7962, 9113, 17049, 6070, 11208, 15692, 746, 11909, 16592, 11515, 11907, 19422, 10737, 14731, 19729, 11315, 14068, 14658, 15294, 16401, 4391, 313, 13264, 18466, 1114, 11158, 747, 11479, 5529, 6280, 6426, 12242, 1347, 16939, 6726, 16593, 9559, 8825, 13181, 2486, 14434, 19066, 18106, 14045, 10875, 16524, 7946, 7665, 4763, 17599, 19208, 5977, 11637, 4085, 13012, 16555, 11215, 14571, 13800, 11669, 3142, 10914, 19469, 19270, 2702, 280, 12345, 67, 5993, 3567, 8758, 1523, 12620, 11451, 10386]\n",
            "Accuracy By Nearest Mean Classifier with Horizontal Histograms Feature: 0.27\n",
            "Accuracy By Nearest Mean Classifier with Vertical Histograms Feature: 0.55\n",
            "Accuracy By Nearest Mean Classifier with Zoning Feature: 0.72\n",
            "Accuracy By Nearest Mean Classifier with Wavelet Feature: 0.73\n",
            "Accuracy By 1 Nearest Neighbor Classifier with Horizontal Histograms Feature: 0.35\n",
            "Accuracy By 1 Nearest Neighbor Classifier with Vertical Histograms Feature: 0.64\n",
            "Accuracy By 1 Nearest Neighbor Classifier with Zoning Feature: 0.77\n",
            "Accuracy By 1 Nearest Neighbor Classifier with Wavelet Feature: 0.73\n",
            "Accuracy By 5 Nearest Neighbor Classifier with Horizontal Histograms Feature: 0.34\n",
            "Accuracy By 5 Nearest Neighbor Classifier with Vertical Histograms Feature: 0.62\n",
            "Accuracy By 5 Nearest Neighbor Classifier with Zoning Feature: 0.74\n",
            "Accuracy By 5 Nearest Neighbor Classifier with Wavelet Feature: 0.69\n",
            "Accuracy By 9 Nearest Neighbor Classifier with Horizontal Histograms Feature: 0.33\n",
            "Accuracy By 9 Nearest Neighbor Classifier with Vertical Histograms Feature: 0.64\n",
            "Accuracy By 9 Nearest Neighbor Classifier with Zoning Feature: 0.72\n",
            "Accuracy By 9 Nearest Neighbor Classifier with Wavelet Feature: 0.67\n",
            "Accuracy by Bayes with Horizontal Histograms Feature: 0.25\n",
            "Accuracy by Bayes with Vertical Histograms Feature: 0.41\n",
            "Accuracy by Bayes with Zoning Feature: 0.46\n",
            "Accuracy by Bayes with Wavelet Feature: 0.6\n",
            "Accuracy by Parzen Window Classifier(h = 0.1) with Horizontal Histograms Feature: 0.27\n",
            "Accuracy by Parzen Window Classifier(h = 0.1) with Vertical Histograms Feature: 0.55\n",
            "Accuracy by Parzen Window Classifier(h = 0.1) with Zoning Feature: 0.72\n",
            "Accuracy by Parzen Window Classifier(h = 0.1) with Wavelet Feature: 0.73\n",
            "Accuracy by Parzen Window Classifier(h = 0.35) with Horizontal Histograms Feature: 0.27\n",
            "Accuracy by Parzen Window Classifier(h = 0.35) with Vertical Histograms Feature: 0.52\n",
            "Accuracy by Parzen Window Classifier(h = 0.35) with Zoning Feature: 0.69\n",
            "Accuracy by Parzen Window Classifier(h = 0.35) with Wavelet Feature: 0.72\n"
          ]
        }
      ],
      "source": [
        "# only 100 train data\n",
        "\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import struct\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "\n",
        "def __convert_to_one_hot(vector, num_classes):\n",
        "    result = np.zeros(shape=[len(vector), num_classes])\n",
        "    result[np.arange(len(vector)), vector] = 1\n",
        "    return result\n",
        "\n",
        "\n",
        "def __resize_image(src_image, dst_image_height, dst_image_width):\n",
        "    src_image_height = src_image.shape[0]\n",
        "    src_image_width = src_image.shape[1]\n",
        "\n",
        "    if src_image_height > dst_image_height or src_image_width > dst_image_width:\n",
        "        height_scale = dst_image_height / src_image_height\n",
        "        width_scale = dst_image_width / src_image_width\n",
        "        scale = min(height_scale, width_scale)\n",
        "        img = cv2.resize(src=src_image, dsize=(0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n",
        "    else:\n",
        "        img = src_image\n",
        "\n",
        "    img_height = img.shape[0]\n",
        "    img_width = img.shape[1]\n",
        "\n",
        "    dst_image = np.zeros(shape=[dst_image_height, dst_image_width], dtype=np.uint8)\n",
        "\n",
        "    y_offset = (dst_image_height - img_height) // 2\n",
        "    x_offset = (dst_image_width - img_width) // 2\n",
        "\n",
        "    dst_image[y_offset:y_offset+img_height, x_offset:x_offset+img_width] = img\n",
        "\n",
        "    return dst_image\n",
        "\n",
        "\n",
        "def read_hoda_cdb(file_name):\n",
        "    with open(file_name, 'rb') as binary_file:\n",
        "\n",
        "        data = binary_file.read()\n",
        "\n",
        "        offset = 0\n",
        "\n",
        "        # read private header\n",
        "\n",
        "        yy = struct.unpack_from('H', data, offset)[0]\n",
        "        offset += 2\n",
        "\n",
        "        m = struct.unpack_from('B', data, offset)[0]\n",
        "        offset += 1\n",
        "\n",
        "        d = struct.unpack_from('B', data, offset)[0]\n",
        "        offset += 1\n",
        "\n",
        "        H = struct.unpack_from('B', data, offset)[0]\n",
        "        offset += 1\n",
        "\n",
        "        W = struct.unpack_from('B', data, offset)[0]\n",
        "        offset += 1\n",
        "\n",
        "        TotalRec = struct.unpack_from('I', data, offset)[0]\n",
        "        offset += 4\n",
        "\n",
        "        LetterCount = struct.unpack_from('128I', data, offset)\n",
        "        offset += 128 * 4\n",
        "\n",
        "        imgType = struct.unpack_from('B', data, offset)[0]  # 0: binary, 1: gray\n",
        "        offset += 1\n",
        "\n",
        "        Comments = struct.unpack_from('256c', data, offset)\n",
        "        offset += 256 * 1\n",
        "\n",
        "        Reserved = struct.unpack_from('245c', data, offset)\n",
        "        offset += 245 * 1\n",
        "\n",
        "        if (W > 0) and (H > 0):\n",
        "            normal = True\n",
        "        else:\n",
        "            normal = False\n",
        "\n",
        "        images = []\n",
        "        labels = []\n",
        "\n",
        "        for i in range(TotalRec):\n",
        "\n",
        "            StartByte = struct.unpack_from('B', data, offset)[0]  # must be 0xff\n",
        "            offset += 1\n",
        "\n",
        "            label = struct.unpack_from('B', data, offset)[0]\n",
        "            offset += 1\n",
        "\n",
        "            if not normal:\n",
        "                W = struct.unpack_from('B', data, offset)[0]\n",
        "                offset += 1\n",
        "\n",
        "                H = struct.unpack_from('B', data, offset)[0]\n",
        "                offset += 1\n",
        "\n",
        "            ByteCount = struct.unpack_from('H', data, offset)[0]\n",
        "            offset += 2\n",
        "\n",
        "            image = np.zeros(shape=[H, W], dtype=np.uint8)\n",
        "\n",
        "            if imgType == 0:\n",
        "                # Binary\n",
        "                for y in range(H):\n",
        "                    bWhite = True\n",
        "                    counter = 0\n",
        "                    while counter < W:\n",
        "                        WBcount = struct.unpack_from('B', data, offset)[0]\n",
        "                        offset += 1\n",
        "                        # x = 0\n",
        "                        # while x < WBcount:\n",
        "                        #     if bWhite:\n",
        "                        #         image[y, x + counter] = 0  # Background\n",
        "                        #     else:\n",
        "                        #         image[y, x + counter] = 255  # ForeGround\n",
        "                        #     x += 1\n",
        "                        if bWhite:\n",
        "                            image[y, counter:counter + WBcount] = 0  # Background\n",
        "                        else:\n",
        "                            image[y, counter:counter + WBcount] = 255  # ForeGround\n",
        "                        bWhite = not bWhite  # black white black white ...\n",
        "                        counter += WBcount\n",
        "            else:\n",
        "                # GrayScale mode\n",
        "                data = struct.unpack_from('{}B'.format(W * H), data, offset)\n",
        "                offset += W * H\n",
        "                image = np.asarray(data, dtype=np.uint8).reshape([W, H]).T\n",
        "\n",
        "            images.append(image)\n",
        "            labels.append(label)\n",
        "\n",
        "        return images, labels\n",
        "\n",
        "\n",
        "def read_hoda_dataset(dataset_path, images_height=32, images_width=32, one_hot=False, reshape=True):\n",
        "    images, labels = read_hoda_cdb(dataset_path)\n",
        "    assert len(images) == len(labels)\n",
        "\n",
        "    X = np.zeros(shape=[len(images), images_height, images_width], dtype=np.float32)\n",
        "    Y = np.zeros(shape=[len(labels)], dtype=np.int)\n",
        "\n",
        "    for i in range(len(images)):\n",
        "        image = images[i]\n",
        "        # Image resizing.\n",
        "        image = __resize_image(src_image=image, dst_image_height=images_height, dst_image_width=images_width)\n",
        "        # Image normalization.\n",
        "        image = image / 255\n",
        "        # Image binarization.\n",
        "        image = np.where(image >= 0.5, 1, 0)\n",
        "        # Image.\n",
        "        X[i] = image\n",
        "        # Label.\n",
        "        Y[i] = labels[i]\n",
        "\n",
        "    if one_hot:\n",
        "        Y = __convert_to_one_hot(Y, 10).astype(dtype=np.float32)\n",
        "    else:\n",
        "        Y = Y.astype(dtype=np.float32)\n",
        "\n",
        "    if reshape:\n",
        "        X = X.reshape(-1, images_height * images_width)\n",
        "    else:\n",
        "        X = X.reshape(-1, images_height, images_width, 1)\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "print('Reading Train 60000.cdb ...')\n",
        "train_images, train_labels = read_hoda_cdb('/content/drive/MyDrive/Colab Notebooks/Train 60000.cdb')\n",
        "print('Train Dataset Loaded')\n",
        "print('Reading Test 20000.cdb ...')\n",
        "test_images, test_labels = read_hoda_cdb('/content/drive/MyDrive/Colab Notebooks/Test 20000.cdb')\n",
        "print('Test Dataset Loaded')\n",
        "\n",
        "\n",
        "\n",
        "import random\n",
        "train_images_subset = train_images[:100]\n",
        "train_labels_subset = train_labels[:100]\n",
        "# Select 100 elements randomly from test_images\n",
        "selected_indices = random.sample(range(len(test_images)), 100)\n",
        "print(selected_indices)\n",
        "# Use the selected indices to get corresponding images and labels\n",
        "test_images_subset = [test_images[i] for i in selected_indices]\n",
        "test_labels_subset = [test_labels[i] for i in selected_indices]\n",
        "\n",
        "\n",
        "# Initialize dimensions\n",
        "max_width, max_width_index = max((img.shape[0], i) for i, img in enumerate(train_images_subset + test_images_subset))\n",
        "max_length, max_length_index = max((img.shape[1], i) for i, img in enumerate(train_images_subset + test_images_subset))\n",
        "# Initialize the 3D train matrix\n",
        "num_images = len(train_images_subset)\n",
        "matrix_3d_train = np.zeros((num_images, max_width, max_length))\n",
        "# Initialize the 3D test matrix\n",
        "num_images1 = len(test_images_subset)\n",
        "matrix_3d_test = np.zeros((num_images1, max_width, max_length))\n",
        "\n",
        "\n",
        "# Fill the matrix with image data, centering each image\n",
        "for i, img in enumerate(train_images_subset):\n",
        "    start_row = (max_width - img.shape[0]) // 2\n",
        "    start_col = (max_length - img.shape[1]) // 2\n",
        "    end_row = start_row + img.shape[0]\n",
        "    end_col = start_col + img.shape[1]\n",
        "    matrix_3d_train[i, start_row:end_row, start_col:end_col] = img\n",
        "\n",
        "# Now, matrix_3d is filled with image data, and each image is centered in its corresponding matrix\n",
        "for i, img in enumerate(test_images_subset):\n",
        "    start_row = (max_width - img.shape[0]) // 2\n",
        "    start_col = (max_length - img.shape[1]) // 2\n",
        "    end_row = start_row + img.shape[0]\n",
        "    end_col = start_col + img.shape[1]\n",
        "    matrix_3d_test[i, start_row:end_row, start_col:end_col] = img\n",
        "\n",
        "\n",
        "\n",
        "# Calculate horizontal and vertical histograms for all images\n",
        "horizontal_histograms_train = np.sum(matrix_3d_train, axis=1)\n",
        "vertical_histograms_train = np.sum(matrix_3d_train, axis=2)\n",
        "# Now, horizontal_histograms and vertical_histograms contain the histograms for all images\n",
        "# Each row in these arrays corresponds to an image in train_images_subset\n",
        "# Calculate horizontal and vertical histograms for all images\n",
        "horizontal_histograms_test = np.sum(matrix_3d_test, axis=1)\n",
        "vertical_histograms_test = np.sum(matrix_3d_test, axis=2)\n",
        "# Now, horizontal_histograms and vertical_histograms contain the histograms for all images\n",
        "# Each row in these arrays corresponds to an image in train_images_subset\n",
        "\n",
        "\n",
        "def zoning(matrix):\n",
        "# Initialize dimensions\n",
        "    num_images, max_width, max_length = matrix.shape\n",
        "\n",
        "# Calculate zoning feature vectors for all images\n",
        "    zoning_feature_vectors = []\n",
        "\n",
        "# Iterate over images\n",
        "    for img_idx in range(num_images):\n",
        "        zoning_feature_vector = []\n",
        "\n",
        "    # Iterate over zones (5x5)\n",
        "        for i in range(5):\n",
        "            for j in range(5):\n",
        "            # Calculate average intensity in the current zone for the current image\n",
        "                avg_intensity = np.mean(matrix[img_idx,\n",
        "                                                        i * max_width // 5 : (i + 1) * max_width // 5,\n",
        "                                                        j * max_length // 5 : (j + 1) * max_length // 5])\n",
        "                zoning_feature_vector.append(avg_intensity)\n",
        "\n",
        "        zoning_feature_vectors.append(zoning_feature_vector)\n",
        "    return zoning_feature_vectors\n",
        "# Now, zoning_feature_vectors is a list containing the zoning feature vector for each image in matrix\n",
        "# Each element of the list corresponds to an image\n",
        "zoning_feature_vectors_train = zoning(matrix_3d_train)\n",
        "zoning_feature_vectors_test = zoning(matrix_3d_test)\n",
        "# Reshape zoning feature vectors to 5x5 matrices\n",
        "zoning_matrices_train = [np.array(feature_vector).reshape(5, 5) for feature_vector in zoning(matrix_3d_train)]\n",
        "zoning_matrices_test = [np.array(feature_vector).reshape(5, 5) for feature_vector in zoning(matrix_3d_test)]\n",
        "\n",
        "\n",
        "import pywt\n",
        "from skimage.transform import resize\n",
        "# Function to resize each image to 64x64\n",
        "def resize_images(images):\n",
        "    resized_images = np.zeros((images.shape[0], 64, 64))\n",
        "    for i in range(images.shape[0]):\n",
        "        resized_images[i] = resize(images[i], (64, 64), mode='constant', anti_aliasing=True)\n",
        "    return resized_images\n",
        "\n",
        "# Function to extract wavelet features\n",
        "def extract_wavelet_features(image):\n",
        "    coeffs = pywt.wavedec2(image, 'haar', level=3)\n",
        "    # Select the approximation coefficients at level 3 (8x8 image)\n",
        "    features = coeffs[0].ravel()\n",
        "    return features\n",
        "\n",
        "# Resize the images\n",
        "resized_images_train = resize_images(matrix_3d_train)\n",
        "\n",
        "# Extract wavelet features for each image\n",
        "wavelet_features_train = np.array([extract_wavelet_features(img) for img in resized_images_train])\n",
        "# wavelet_features now contains the desired features for each image\n",
        "# Its shape should be (3000, 64)\n",
        "# Resize the images\n",
        "resized_images_test = resize_images(matrix_3d_test)\n",
        "\n",
        "# Extract wavelet features for each image\n",
        "wavelet_features_test = np.array([extract_wavelet_features(img) for img in resized_images_test])\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# Assuming you have imported the necessary libraries and have your data loaded in the variables mentioned\n",
        "\n",
        "# Normalizing ndarrays\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "horizontal_histograms_train = scaler.fit_transform(horizontal_histograms_train)\n",
        "vertical_histograms_train = scaler.fit_transform(vertical_histograms_train)\n",
        "\n",
        "horizontal_histograms_test  = scaler.fit_transform(horizontal_histograms_test)\n",
        "vertical_histograms_test = scaler.fit_transform(vertical_histograms_test)\n",
        "\n",
        "wavelet_features_train = scaler.fit_transform(wavelet_features_train)\n",
        "wavelet_features_test = scaler.fit_transform(wavelet_features_test)\n",
        "\n",
        "# Normalizing lists\n",
        "zoning_feature_vectors_train = [scaler.fit_transform(np.array(vec).reshape(-1, 1)) for vec in zoning_feature_vectors_train]\n",
        "zoning_feature_vectors_test = [scaler.fit_transform(np.array(vec).reshape(-1, 1)) for vec in zoning_feature_vectors_test]\n",
        "\n",
        "\n",
        "class NearestMeanClassifier:\n",
        "    def __init__(self):\n",
        "        self.class_means = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        unique_classes = np.unique(y)\n",
        "        self.class_means = {}\n",
        "\n",
        "        for class_label in unique_classes:\n",
        "            class_instances = X[y == class_label]\n",
        "            class_mean = np.mean(class_instances, axis=0)\n",
        "            self.class_means[class_label] = class_mean\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.class_means is None:\n",
        "            raise RuntimeError(\"The classifier has not been trained. Call fit() first.\")\n",
        "\n",
        "        predictions = []\n",
        "        for instance in X:\n",
        "            nearest_class = min(self.class_means.keys(), key=lambda x: np.linalg.norm(instance - self.class_means[x]))\n",
        "            predictions.append(nearest_class)\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "# Example usage:\n",
        "# Assuming X_train, y_train, X_test are your training features, training labels, and test features\n",
        "# X_train and X_test should be NumPy arrays\n",
        "\n",
        "def NMC(FTr, LTr, FTe, LTe, str):\n",
        "# Create and train the classifier\n",
        "    classifier = NearestMeanClassifier()\n",
        "    classifier.fit(FTr, LTr)\n",
        "# Make predictions on the test set\n",
        "    predictions = classifier.predict(FTe)\n",
        "# Evaluate the accuracy or other metrics\n",
        "    accuracy = np.mean(predictions == LTe)\n",
        "    print(f\"Accuracy By Nearest Mean Classifier with {str}: {accuracy}\")\n",
        "\n",
        "NMC(horizontal_histograms_train, train_labels_subset, horizontal_histograms_test, test_labels_subset, 'Horizontal Histograms Feature')\n",
        "NMC(vertical_histograms_train, train_labels_subset, vertical_histograms_test, test_labels_subset, 'Vertical Histograms Feature')\n",
        "NMC(np.array(zoning_feature_vectors_train), train_labels_subset, np.array(zoning_feature_vectors_test), test_labels_subset, 'Zoning Feature')\n",
        "NMC(wavelet_features_train, train_labels_subset, wavelet_features_test, test_labels_subset, 'Wavelet Feature')\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = [self._predict(x) for x in X]\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        distances = [np.linalg.norm(x - x_train) for x_train in self.X_train]\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "        most_common = Counter(k_nearest_labels).most_common(1)\n",
        "        return most_common[0][0]\n",
        "\n",
        "def KNNC(FTr, LTr, FTe, LTe, str, k=3):\n",
        "    # Create KNN classifier\n",
        "    knn = KNN(k)\n",
        "    knn.fit(FTr, LTr)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = knn.predict(FTe)\n",
        "    # Evaluate the accuracy or other metrics\n",
        "    accuracy = np.mean(predictions == LTe)\n",
        "    print(f\"Accuracy By {k} Nearest Neighbor Classifier with {str}: {accuracy}\")\n",
        "\n",
        "\n",
        "KNNC(horizontal_histograms_train, train_labels_subset, horizontal_histograms_test, test_labels_subset, 'Horizontal Histograms Feature', 1)\n",
        "KNNC(vertical_histograms_train, train_labels_subset, vertical_histograms_test, test_labels_subset, 'Vertical Histograms Feature', 1)\n",
        "KNNC(np.array(zoning_feature_vectors_train), train_labels_subset, np.array(zoning_feature_vectors_test), test_labels_subset, 'Zoning Feature', 1)\n",
        "KNNC(wavelet_features_train, train_labels_subset, wavelet_features_test, test_labels_subset, 'Wavelet Feature', 1)\n",
        "\n",
        "KNNC(horizontal_histograms_train, train_labels_subset, horizontal_histograms_test, test_labels_subset, 'Horizontal Histograms Feature', 5)\n",
        "KNNC(vertical_histograms_train, train_labels_subset, vertical_histograms_test, test_labels_subset, 'Vertical Histograms Feature', 5)\n",
        "KNNC(np.array(zoning_feature_vectors_train), train_labels_subset, np.array(zoning_feature_vectors_test), test_labels_subset, 'Zoning Feature', 5)\n",
        "KNNC(wavelet_features_train, train_labels_subset, wavelet_features_test, test_labels_subset, 'Wavelet Feature', 5)\n",
        "\n",
        "KNNC(horizontal_histograms_train, train_labels_subset, horizontal_histograms_test, test_labels_subset, 'Horizontal Histograms Feature', 9)\n",
        "KNNC(vertical_histograms_train, train_labels_subset, vertical_histograms_test, test_labels_subset, 'Vertical Histograms Feature', 9)\n",
        "KNNC(np.array(zoning_feature_vectors_train), train_labels_subset, np.array(zoning_feature_vectors_test), test_labels_subset, 'Zoning Feature', 9)\n",
        "KNNC(wavelet_features_train, train_labels_subset, wavelet_features_test, test_labels_subset, 'Wavelet Feature', 9)\n",
        "\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "def GBC(FTr, LTr, FTe, LTe, str):\n",
        "    # Create a Gaussian Naive Bayes classifier\n",
        "    model = GaussianNB()\n",
        "    # Train the model on the training set\n",
        "    model.fit(FTr, LTr)\n",
        "    # Make predictions on the testing set\n",
        "    y_pred = model.predict(FTe)\n",
        "    # Evaluate the performance of the classifier\n",
        "    accuracy = metrics.accuracy_score(LTe, y_pred)\n",
        "    print(f\"Accuracy by Bayes with {str}: {accuracy}\")\n",
        "\n",
        "GBC(horizontal_histograms_train, train_labels_subset, horizontal_histograms_test, test_labels_subset, 'Horizontal Histograms Feature')\n",
        "GBC(vertical_histograms_train, train_labels_subset, vertical_histograms_test, test_labels_subset, 'Vertical Histograms Feature')\n",
        "GBC(np.array(zoning_feature_vectors_train).reshape(len(zoning_feature_vectors_train), -1), train_labels_subset, np.array(zoning_feature_vectors_test).reshape(len(zoning_feature_vectors_test), -1), test_labels_subset, 'Zoning Feature')\n",
        "GBC(wavelet_features_train, train_labels_subset, wavelet_features_test, test_labels_subset, 'Wavelet Feature')\n",
        "\n",
        "\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "class ParzenWindowClassifier:\n",
        "    def __init__(self, h=1.0):\n",
        "        self.h = h  # Bandwidth parameter\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.classes = np.unique(y_train)\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        predictions = [self._predict(x) for x in X_test]\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        likelihoods = []\n",
        "\n",
        "        for c in self.classes:\n",
        "            class_indices = np.where(self.y_train == c)[0]\n",
        "            class_data = self.X_train[class_indices]\n",
        "\n",
        "            # Calculate the Parzen window estimate for the class\n",
        "            likelihood = np.sum(self._parzen_window(x, class_data)) / len(class_data)\n",
        "            likelihoods.append(likelihood)\n",
        "\n",
        "        # Return the class with the highest likelihood\n",
        "        predicted_class = self.classes[np.argmax(likelihoods)]\n",
        "        return predicted_class\n",
        "\n",
        "    def _parzen_window(self, x, data):\n",
        "        # Use a multivariate normal distribution as the window function\n",
        "        window_function = multivariate_normal(mean=np.mean(data, axis=0), cov=np.eye(data.shape[1]) * self.h**2)\n",
        "        return window_function.pdf(x)\n",
        "\n",
        "def PWC(FTr, LTr, FTe, LTe, str, h = 1.0):\n",
        "    # Create Parzen Window classifier\n",
        "    parzen_classifier = ParzenWindowClassifier(h)\n",
        "    parzen_classifier.fit(FTr, LTr)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = parzen_classifier.predict(FTe)\n",
        "    # Evaluate the performance of the classifier\n",
        "    accuracy = metrics.accuracy_score(LTe, y_pred)\n",
        "    print(f\"Accuracy by Parzen Window Classifier(h = {h}) with {str}: {accuracy}\")\n",
        "\n",
        "\n",
        "PWC(horizontal_histograms_train, train_labels_subset, horizontal_histograms_test, test_labels_subset, 'Horizontal Histograms Feature', 0.1)\n",
        "PWC(vertical_histograms_train, train_labels_subset, vertical_histograms_test, test_labels_subset, 'Vertical Histograms Feature', 0.1)\n",
        "PWC(np.array(zoning_feature_vectors_train).reshape(len(zoning_feature_vectors_train), -1), train_labels_subset, np.array(zoning_feature_vectors_test).reshape(len(zoning_feature_vectors_test), -1), test_labels_subset, 'Zoning Feature', 0.1)\n",
        "PWC(wavelet_features_train, train_labels_subset, wavelet_features_test, test_labels_subset, 'Wavelet Feature', 0.1)\n",
        "\n",
        "\n",
        "PWC(horizontal_histograms_train, train_labels_subset, horizontal_histograms_test, test_labels_subset, 'Horizontal Histograms Feature', 0.35)\n",
        "PWC(vertical_histograms_train, train_labels_subset, vertical_histograms_test, test_labels_subset, 'Vertical Histograms Feature', 0.35)\n",
        "PWC(np.array(zoning_feature_vectors_train).reshape(len(zoning_feature_vectors_train), -1), train_labels_subset, np.array(zoning_feature_vectors_test).reshape(len(zoning_feature_vectors_test), -1), test_labels_subset, 'Zoning Feature', 0.35)\n",
        "PWC(wavelet_features_train, train_labels_subset, wavelet_features_test, test_labels_subset, 'Wavelet Feature', 0.35)\n",
        "\n"
      ]
    }
  ]
}