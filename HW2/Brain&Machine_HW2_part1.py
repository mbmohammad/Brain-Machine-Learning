# -*- coding: utf-8 -*-
"""Untitled21.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16zsSG3BmujWW9UQN-fW9mU-dpx4NcZI4
"""

import cv2
import os
import pandas as pd
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt
import numpy as np

# Specify the root path to your dataset directory
root_path = '/content/drive/MyDrive/Colab Notebooks/4- Classification with one hidden layer (optional)/DataSet'

# Create a directory to save the images
output_directory = '/content/drive/MyDrive/Colab Notebooks/4- Classification with one hidden layer (optional)/DataSet/all_images'
os.makedirs(output_directory, exist_ok=True)

# Create an empty list to store images
all_images = []

# Loop through each directory (s1, s2, ..., s40)
for i in range(1, 41):
    # Construct the path to the current directory
    current_dir = os.path.join(root_path, f's{i}')

    # Get a list of all files in the current directory
    files = os.listdir(current_dir)

    # Loop through each file in the directory
    for file in files:
        # Construct the full file path
        file_path = os.path.join(current_dir, file)

        # Read the image using OpenCV in grayscale
        img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)

        # Resize the image to 48x48 pixels
        img_resized = cv2.resize(img, (48, 48))

        # Save the resized image to the output directory
        output_path = os.path.join(output_directory, f's{i}_{file}')
        cv2.imwrite(output_path, img_resized)

        # Append the resized image to the list
        all_images.append(img_resized)

# Convert the list of images to a DataFrame
df = pd.DataFrame({'Image': all_images})

# Calculate the number of rows and columns for subplots
num_rows = 10
num_cols = 40

# Create a subplot with a grid layout
fig, axs = plt.subplots(num_rows, num_cols, figsize=(40, 10))

# Loop through each image and display it in a subplot
for i in range(len(all_images)):
    row = i % num_rows
    col = i // num_rows
    axs[row, col].imshow(all_images[i], cmap='gray')  # Use cmap='gray' for grayscale images
    axs[row, col].axis('off')

# Adjust layout for better display
plt.tight_layout()
plt.show()

# Create empty lists for train and test data
train_data = []
test_data = []

# Number of images per subject for train and test
num_train_images = 5
num_test_images = 5

# Loop through each subject
for i in range(1, 41):
    # Extract images for train data
    train_images = all_images[(i - 1) * 10 : (i - 1) * 10 + num_train_images]
    train_data.extend(train_images)

    # Extract images for test data
    test_images = all_images[(i - 1) * 10 + num_train_images : i * 10]
    test_data.extend(test_images)

# Convert train and test data to DataFrames
df_train = pd.DataFrame({'Image': train_data})
df_test = pd.DataFrame({'Image': test_data})

num_images_to_plot = 40

# Plot the first 40 images
fig, axes = plt.subplots(4, 10, figsize=(15, 6), subplot_kw={'xticks':[], 'yticks':[]})

for i, ax in enumerate(axes.flat):
    ax.imshow(train_data[i], cmap='gray')  # Assuming images are grayscale
    ax.set_title(f"Image {i+1}")

plt.show()

# Number of subjects
num_subjects = 40

# Number of images per subject for train and test
num_train_images = 5
num_test_images = 5

# Create Y_train and Y_test arrays
Y_train = np.repeat(np.arange(1, num_subjects + 1), num_train_images)
Y_test = np.repeat(np.arange(1, num_subjects + 1), num_test_images)

# Verify the shapes
print("Y_train shape:", Y_train.shape)
print("Y_test shape:", Y_test.shape)

image_shape = all_images[0].shape
print("Shape of each image:", image_shape)

import numpy as np

# Function to flatten each image to a vector
def flatten_image(image):
    return image.flatten()

# Apply the flatten_image function to each image in train_data and test_data
train_vectors = [flatten_image(image) for image in train_data]
test_vectors = [flatten_image(image) for image in test_data]

# Convert the lists of vectors to NumPy arrays
x_train_reshaped = np.array(train_vectors)
x_test_reshaped = np.array(test_vectors)

#calculate the covariance matrix and the eigenvalue and eigenvectors of the covariance matrix.
covariance = np.cov(x_train_reshaped.T)

explained_variance = 0.87
U, S, Vh = np.linalg.svd(covariance, full_matrices=True)
#calculate the total variance from eigenvalues and find the first k component that contains the eplained_variance of the total variance.
NormSumOfS = np.cumsum(S)/np.sum(S)
index = np.argmax(NormSumOfS > explained_variance)
V = Vh.T

# here show 50 first of this images
def show_images(num_images,X):
    #inputs dataset and number of images wants to show
    #output plot images
    plt.figure(figsize=(10, 5))
    for i in range(num_images):
        plt.subplot(5, 10, i + 1)
        plt.imshow(X[i] / 255.0, cmap='gray')
        plt.axis('off')
    plt.show()
#Project Images into Reduced Dimensionality Eigenbasis F=X*V (X is our dataset,F our reduced dimensionality dataset,V is k choosen eigenvectors)
#And Reproject Images to Standard Basis for plotting the reduced component image X_k=F*V_transpose (V_transpose is equal to V inverse)
#select one sample and reshape it to a 28 by 28 matrix and plot them as image
F = np.dot(np.dot(x_train_reshaped,V[:, :index]), V[:, :index].T)
#plot the dimentionally reduced data
#plot the original data
F_reshaped = F.reshape(200, 48, 48)
show_images(1,F_reshaped)
show_images(1,train_data)

from sklearn.decomposition import PCA

def do_pca(n_components, data, data1):
    # Create a PCA instance with the desired number of components
    pca = PCA(n_components=n_components)

    # Fit the PCA model to your data and transform the data
    projected_data = pca.fit_transform(data)
    projected_data1 = pca.transform(data1)
    return projected_data, projected_data1


import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.ensemble import RandomForestClassifier

def plot_confusion_matrix(Y_test, Y_pred):
    cm = confusion_matrix(Y_test, Y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()

def ML_model(X_train, X_test, Y_train, Y_test, print_output=True, pc=1, plot_conf=False):
    # You can configure the model by setting hyperparameters such as the number of trees, max depth, etc.
    model = RandomForestClassifier(n_estimators=100)  # For classification
    # model = RandomForestRegressor(n_estimators=100)  # For regression

    # Fit the model on the training data
    model.fit(X_train, Y_train)

    # Make predictions on the test data
    Y_pred = model.predict(X_test)

    # Calculate the accuracy of the model
    acc = accuracy_score(Y_test, Y_pred)

    if print_output:
        print(f"Accuracy: {acc * 100:.2f}%")
        print(f"PC Components: {pc}")
    if plot_conf:
        # Plot confusion matrix
        plot_confusion_matrix(Y_test, Y_pred)

    return acc

import matplotlib.pyplot as plt

# Initialize lists to store accuracy and number of components
acc_list, pc_list = [], []

for pc in range(2, 70):
    # Perform PCA with the current number of components
    projected_train_data, projected_test_data = do_pca(pc, x_train_reshaped, x_test_reshaped)

    # Train the SVM model and calculate accuracy
    accuracy = ML_model(projected_train_data, projected_test_data, Y_train, Y_test, True, pc)

    # Append the accuracy and number of components to the lists
    acc_list.append(accuracy)
    pc_list.append(pc)

# Plot the accuracy vs. number of components
plt.figure(figsize=(10, 6))
plt.plot(pc_list, acc_list, marker='o', linestyle='-')
plt.title('Accuracy vs. Number of Components')
plt.xlabel('Number of Components')
plt.ylabel('Accuracy')
plt.grid(True)
plt.show()

# Find the index of the maximum accuracy in the acc_list
best_acc_index = acc_list.index(max(acc_list))

# Get the corresponding number of components for the best accuracy
best_pc = pc_list[best_acc_index]

print("Number of components for the best model:", best_pc)

# Perform PCA with the current number of components
projected_train_data, projected_test_data = do_pca(pc, x_train_reshaped, x_test_reshaped)

# Train the SVM model and calculate accuracy
accuracy = ML_model(projected_train_data, projected_test_data, Y_train, Y_test, True, best_pc, True)

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, mutual_info_classif


def select_features_after_pca(n_components, data, data1, k):
    # Create a PCA instance with the desired number of components
    pca = PCA(n_components=n_components)

    # Fit the PCA model to your data and transform the data
    projected_data = pca.fit_transform(data)
    projected_data1 = pca.transform(data1)

    # Train a RandomForestClassifier on the reduced data to obtain feature importances
    model = RandomForestClassifier(n_estimators=100)
    model.fit(projected_data, Y_train)

    # Get feature importances
    feature_importances = model.feature_importances_

    # Rank features based on importance
    ranked_features = np.argsort(feature_importances)[::-1]

    # Select the top-k features using Information Gain
    k_best = SelectKBest(mutual_info_classif, k=k)
    selected_data = k_best.fit_transform(projected_data, Y_train)
    selected_data1 = k_best.transform(projected_data1)

    return selected_data, selected_data1

# ... (The rest of your code remains unchanged)

# Initialize lists to store accuracy and number of components
acc_list, pc_list = [], []

# Specify the number of features (k) to select after PCA
k_features = 20  # You can adjust this based on your requirements

for pc in range(5, 70):
    # Perform PCA with the current number of components and select features
    selected_train_data, selected_test_data = select_features_after_pca(pc, x_train_reshaped, x_test_reshaped, int(round(pc / 2)))

    # Train the SVM model and calculate accuracy
    accuracy = ML_model(selected_train_data, selected_test_data, Y_train, Y_test, True, pc)

    # Append the accuracy and number of components to the lists
    acc_list.append(accuracy)
    pc_list.append(pc)

# Plot the accuracy vs. number of components
plt.figure(figsize=(10, 6))
plt.plot(pc_list, acc_list, marker='o', linestyle='-')
plt.title('Accuracy vs. Number of Components with Feature Selection(Information Gain), # of features = round(pc / 2)')
plt.xlabel('Number of Components')
plt.ylabel('Accuracy')
plt.grid(True)
plt.show()
# Find the index of the maximum accuracy in the acc_list
best_acc_index = acc_list.index(max(acc_list))

# Get the corresponding number of components for the best accuracy
best_pc = pc_list[best_acc_index]

# Perform PCA with the current number of components
projected_train_data, projected_test_data = select_features_after_pca(pc, x_train_reshaped, x_test_reshaped, int(round(best_pc / 2)))

# Train the SVM model and calculate accuracy
accuracy = ML_model(projected_train_data, projected_test_data, Y_train, Y_test, True, best_pc, True)

!pip install skfeature-chappers

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest
from skfeature.function.similarity_based import fisher_score

def select_features_after_pca2(n_components, data, data1, k):
    # Create a PCA instance with the desired number of components
    pca = PCA(n_components=n_components)

    # Fit the PCA model to your data and transform the data
    projected_data = pca.fit_transform(data)
    projected_data1 = pca.transform(data1)

    # Train a RandomForestClassifier on the reduced data to obtain feature importances
    model = RandomForestClassifier(n_estimators=100)
    model.fit(projected_data, Y_train)

    # Get feature importances
    feature_importances = model.feature_importances_

    # Rank features based on importance
    ranked_features = np.argsort(feature_importances)[::-1]

    # Select the top-k features using Fisher's Score
    score = fisher_score.fisher_score(projected_data, Y_train)
    ranked_features_fisher = np.argsort(score)[::-1]
    selected_data = projected_data[:, ranked_features_fisher[:k]]
    selected_data1 = projected_data1[:, ranked_features_fisher[:k]]

    return selected_data, selected_data1

# ... (The rest of your code remains unchanged)

# Initialize lists to store accuracy and number of components
acc_list, pc_list = [], []

# Specify the number of features (k) to select after PCA
k_features = 20  # You can adjust this based on your requirements

for pc in range(5, 70):
    # Perform PCA with the current number of components and select features
    selected_train_data, selected_test_data = select_features_after_pca2(pc, x_train_reshaped, x_test_reshaped, int(round(pc / 2)))

    # Train the SVM model and calculate accuracy
    accuracy = ML_model(selected_train_data, selected_test_data, Y_train, Y_test, True, pc)

    # Append the accuracy and number of components to the lists
    acc_list.append(accuracy)
    pc_list.append(pc)

# Plot the accuracy vs. number of components
plt.figure(figsize=(10, 6))
plt.plot(pc_list, acc_list, marker='o', linestyle='-')
plt.title('Accuracy vs. Number of Components with Feature Selection(Fisher Score), # of features = round(pc / 2)')
plt.xlabel('Number of Components')
plt.ylabel('Accuracy')
plt.grid(True)
plt.show()
# Find the index of the maximum accuracy in the acc_list
best_acc_index = acc_list.index(max(acc_list))

# Get the corresponding number of components for the best accuracy
best_pc = pc_list[best_acc_index]

# Perform PCA with the current number of components
projected_train_data, projected_test_data = select_features_after_pca2(pc, x_train_reshaped, x_test_reshaped, int(round(best_pc / 2)))

# Train the SVM model and calculate accuracy
accuracy = ML_model(projected_train_data, projected_test_data, Y_train, Y_test, True, best_pc, True)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
from scipy.stats import pearsonr

def select_features_after_pca3(n_components, data, data1, k):
    # Create a PCA instance with the desired number of components
    pca = PCA(n_components=n_components)

    # Fit the PCA model to your data and transform the data
    projected_data = pca.fit_transform(data)
    projected_data1 = pca.transform(data1)

    # Train a RandomForestClassifier on the reduced data to obtain feature importances
    model = RandomForestClassifier(n_estimators=100)
    model.fit(projected_data, Y_train)

    # Get feature importances
    feature_importances = model.feature_importances_

    # Rank features based on importance
    ranked_features = np.argsort(feature_importances)[::-1]

    # Calculate the Pearson correlation coefficients between features and target (Y_train)
    correlation_coefficients = [pearsonr(projected_data[:, i], Y_train)[0] for i in range(projected_data.shape[1])]

    # Rank features based on absolute correlation coefficients
    ranked_features_correlation = np.argsort(np.abs(correlation_coefficients))[::-1]

    # Select the top-k features using Correlation Coefficient
    selected_data = projected_data[:, ranked_features_correlation[:k]]
    selected_data1 = projected_data1[:, ranked_features_correlation[:k]]

    return selected_data, selected_data1

# ... (The rest of your code remains unchanged)

# Initialize lists to store accuracy and number of components
acc_list, pc_list = [], []

# Specify the number of features (k) to select after PCA
k_features = 20  # You can adjust this based on your requirements

for pc in range(5, 70):
    # Perform PCA with the current number of components and select features
    selected_train_data, selected_test_data = select_features_after_pca3(pc, x_train_reshaped, x_test_reshaped, int(round(pc / 2)))

    # Train the SVM model and calculate accuracy
    accuracy = ML_model(selected_train_data, selected_test_data, Y_train, Y_test, True, pc)

    # Append the accuracy and number of components to the lists
    acc_list.append(accuracy)
    pc_list.append(pc)

# Plot the accuracy vs. number of components
plt.figure(figsize=(10, 6))
plt.plot(pc_list, acc_list, marker='o', linestyle='-')
plt.title('Accuracy vs. Number of Components with Feature Selection(Correlation Coeff), # of features = round(pc / 2)')
plt.xlabel('Number of Components')
plt.ylabel('Accuracy')
plt.grid(True)
plt.show()
# Find the index of the maximum accuracy in the acc_list
best_acc_index = acc_list.index(max(acc_list))

# Get the corresponding number of components for the best accuracy
best_pc = pc_list[best_acc_index]

# Perform PCA with the current number of components
projected_train_data, projected_test_data = select_features_after_pca3(pc, x_train_reshaped, x_test_reshaped, int(round(best_pc / 2)))

# Train the SVM model and calculate accuracy
accuracy = ML_model(projected_train_data, projected_test_data, Y_train, Y_test, True, best_pc, True)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
from sklearn.feature_selection import VarianceThreshold

def select_features_after_pca4(n_components, data, data1, k, variance_threshold=0.1):
    # Create a PCA instance with the desired number of components
    pca = PCA(n_components=n_components)

    # Fit the PCA model to your data and transform the data
    projected_data = pca.fit_transform(data)
    projected_data1 = pca.transform(data1)

    # Train a RandomForestClassifier on the reduced data to obtain feature importances
    model = RandomForestClassifier(n_estimators=100)
    model.fit(projected_data, Y_train)

    # Get feature importances
    feature_importances = model.feature_importances_

    # Rank features based on importance
    ranked_features = np.argsort(feature_importances)[::-1]

    # Apply Variance Threshold to select features
    selector = VarianceThreshold(threshold=variance_threshold)
    selected_data = selector.fit_transform(projected_data)
    selected_data1 = selector.transform(projected_data1)

    # If you want to keep the top-k features after applying Variance Threshold
    if k is not None:
        top_k_indices = np.argsort(feature_importances)[::-1][:k]
        selected_data = selected_data[:, top_k_indices]
        selected_data1 = selected_data1[:, top_k_indices]

    return selected_data, selected_data1

# ... (The rest of your code remains unchanged)
# Initialize lists to store accuracy and number of components
acc_list, pc_list = [], []

# Specify the number of features (k) to select after PCA
k_features = 20  # You can adjust this based on your requirements

for pc in range(5, 70):
    # Perform PCA with the current number of components and select features
    selected_train_data, selected_test_data = select_features_after_pca4(pc, x_train_reshaped, x_test_reshaped, int(round(pc / 2)))

    # Train the SVM model and calculate accuracy
    accuracy = ML_model(selected_train_data, selected_test_data, Y_train, Y_test, True, pc)

    # Append the accuracy and number of components to the lists
    acc_list.append(accuracy)
    pc_list.append(pc)

# Plot the accuracy vs. number of components
plt.figure(figsize=(10, 6))
plt.plot(pc_list, acc_list, marker='o', linestyle='-')
plt.title('Accuracy vs. Number of Components with Feature Selection(Variance Threshold), # of features = round(pc / 2)')
plt.xlabel('Number of Components')
plt.ylabel('Accuracy')
plt.grid(True)
plt.show()
# Find the index of the maximum accuracy in the acc_list
best_acc_index = acc_list.index(max(acc_list))

# Get the corresponding number of components for the best accuracy
best_pc = pc_list[best_acc_index]

# Perform PCA with the current number of components
projected_train_data, projected_test_data = select_features_after_pca4(pc, x_train_reshaped, x_test_reshaped, int(round(best_pc / 2)))

# Train the SVM model and calculate accuracy
accuracy = ML_model(projected_train_data, projected_test_data, Y_train, Y_test, True, best_pc, True)